{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**MULTILINGUAL NLU SYSTEM - INTENT + SLOT FILLING**"
      ],
      "metadata": {
        "id": "n4ndD-pVsOk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "264iUqyEr2mr",
        "outputId": "07955d4d-06c0-4ddf-961b-2006ce13bb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU Info:\n",
            "Device: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 42.5 GB\n",
            "\n",
            " Setup complete!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch safetensors\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Check GPU\n",
        "print(f\"\\nGPU Info:\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"\\n Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Loading Intent Classification Model ===\")\n",
        "\n",
        "drive_path = '/content/drive/MyDrive/intent_project'\n",
        "intent_model_dir = f'{drive_path}/xlm-roberta-intent-classifier-final'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load with optimizations\n",
        "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_dir)\n",
        "intent_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    intent_model_dir,\n",
        "    torch_dtype=torch.float16,  # Use half precision for A100\n",
        "    device_map=\"auto\"            # Automatic device placement\n",
        ")\n",
        "intent_model.eval()\n",
        "\n",
        "# Load intent mappings\n",
        "with open(f'{intent_model_dir}/intent2id.json', 'r') as f:\n",
        "    intent2id = json.load(f)\n",
        "with open(f'{intent_model_dir}/id2intent.json', 'r') as f:\n",
        "    id2intent = json.load(f)\n",
        "\n",
        "print(f\"✓ Intent model loaded ({len(id2intent)} intents)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV0UuA12se-_",
        "outputId": "d1793551-41d6-4f08-bb68-b03b3849879f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Loading Intent Classification Model ===\n",
            "✓ Intent model loaded (60 intents)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Loading Slot Filling Model ===\")\n",
        "\n",
        "slot_model_dir = f'{drive_path}/slot_filling_model/final_model'\n",
        "\n",
        "# Load with optimizations\n",
        "slot_tokenizer = AutoTokenizer.from_pretrained(slot_model_dir)\n",
        "slot_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    slot_model_dir,\n",
        "    torch_dtype=torch.float16,  # Use half precision\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "slot_model.eval()\n",
        "\n",
        "# Load slot mappings\n",
        "with open(f'{drive_path}/slot_filling_model/slot2id.json', 'r') as f:\n",
        "    slot2id = json.load(f)\n",
        "with open(f'{drive_path}/slot_filling_model/id2slot.json', 'r') as f:\n",
        "    id2slot = json.load(f)\n",
        "\n",
        "print(f\"✓ Slot model loaded ({len(id2slot)} slot tags)\")\n",
        "print(f\"✓ Using mixed precision (float16) for A100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjZcb8T8tLOA",
        "outputId": "3cf7e329-e739-4773-e62d-9ce692c4593e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Loading Slot Filling Model ===\n",
            "✓ Slot model loaded (111 slot tags)\n",
            "✓ Using mixed precision (float16) for A100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()  # Context manager for inference optimization\n",
        "def predict_intent(utterance, tokenizer, model, id2intent, device):\n",
        "    \"\"\"Predict intent for utterance\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        utterance,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        padding='max_length'\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.cuda.amp.autocast():  # Enable autocasting for A100\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        pred_id = torch.argmax(logits, dim=-1).item()\n",
        "        confidence = torch.softmax(logits, dim=-1).squeeze()[pred_id].item()\n",
        "\n",
        "    intent = id2intent[str(pred_id)]\n",
        "    return intent, confidence"
      ],
      "metadata": {
        "id": "AVTm5GUItSWP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def extract_slots(utterance, tokenizer, model, id2slot, device):\n",
        "    \"\"\"Extract slot entities from utterance\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        utterance,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        padding='max_length'\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().cpu().numpy())\n",
        "\n",
        "    # Decode BIO tags\n",
        "    slots_raw = []\n",
        "    current_slot_type = None\n",
        "    current_slot_tokens = []\n",
        "\n",
        "    for token, pred_id in zip(tokens, predictions):\n",
        "        slot_label = id2slot[str(int(pred_id))]\n",
        "\n",
        "        if token in ['<s>', '</s>', '<pad>']:\n",
        "            if current_slot_type and current_slot_tokens:\n",
        "                slots_raw.append({'type': current_slot_type, 'tokens': current_slot_tokens})\n",
        "            current_slot_type = None\n",
        "            current_slot_tokens = []\n",
        "            continue\n",
        "\n",
        "        if slot_label.startswith('B-'):\n",
        "            if current_slot_type and current_slot_tokens:\n",
        "                slots_raw.append({'type': current_slot_type, 'tokens': current_slot_tokens})\n",
        "            current_slot_type = slot_label[2:]\n",
        "            current_slot_tokens = [token]\n",
        "        elif slot_label.startswith('I-'):\n",
        "            slot_type = slot_label[2:]\n",
        "            if slot_type == current_slot_type and current_slot_type:\n",
        "                current_slot_tokens.append(token)\n",
        "            else:\n",
        "                if current_slot_type and current_slot_tokens:\n",
        "                    slots_raw.append({'type': current_slot_type, 'tokens': current_slot_tokens})\n",
        "                current_slot_type = slot_type\n",
        "                current_slot_tokens = [token]\n",
        "        elif slot_label == 'O':\n",
        "            if current_slot_type and current_slot_tokens:\n",
        "                slots_raw.append({'type': current_slot_type, 'tokens': current_slot_tokens})\n",
        "            current_slot_type = None\n",
        "            current_slot_tokens = []\n",
        "\n",
        "    if current_slot_type and current_slot_tokens:\n",
        "        slots_raw.append({'type': current_slot_type, 'tokens': current_slot_tokens})\n",
        "\n",
        "    # Post-process: merge subword tokens\n",
        "    slots_merged = []\n",
        "    i = 0\n",
        "    while i < len(slots_raw):\n",
        "        current = slots_raw[i]\n",
        "        merged_tokens = list(current['tokens'])\n",
        "        j = i + 1\n",
        "        while j < len(slots_raw) and slots_raw[j]['type'] == current['type']:\n",
        "            next_token = slots_raw[j]['tokens'][0]\n",
        "            if not next_token.startswith('▁'):\n",
        "                merged_tokens.extend(slots_raw[j]['tokens'])\n",
        "                j += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        slot_value = ''\n",
        "        for token in merged_tokens:\n",
        "            if token.startswith('▁'):\n",
        "                if slot_value:\n",
        "                    slot_value += ' '\n",
        "                slot_value += token[1:]\n",
        "            else:\n",
        "                slot_value += token\n",
        "\n",
        "        slot_value = slot_value.strip()\n",
        "        if slot_value:\n",
        "            slots_merged.append((current['type'], slot_value))\n",
        "        i = j\n",
        "\n",
        "    return slots_merged"
      ],
      "metadata": {
        "id": "fR8B2FrZuykt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 6: Combined NLU Function\n",
        "# ============================================================\n",
        "\n",
        "def process_utterance(utterance):\n",
        "    \"\"\"Process utterance: extract intent and slots\"\"\"\n",
        "    intent, confidence = predict_intent(utterance, intent_tokenizer, intent_model, id2intent, device)\n",
        "    slots = extract_slots(utterance, slot_tokenizer, slot_model, id2slot, device)\n",
        "\n",
        "    return {\n",
        "        'utterance': utterance,\n",
        "        'intent': intent,\n",
        "        'confidence': confidence,\n",
        "        'slots': slots\n",
        "    }"
      ],
      "metadata": {
        "id": "K2XLWKd4u5yS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 7: Test Combined System\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TESTING COMBINED NLU SYSTEM (A100)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_utterances = [\n",
        "    \"Wake me up at 6 AM tomorrow\",\n",
        "    \"Book a table at an Italian restaurant at 7pm\",\n",
        "    \"Play some jazz music by Miles Davis\",\n",
        "    \"Turn off the bedroom lights\",\n",
        "    \"Remind me to call John next Friday\",\n",
        "    \"What's the weather in Paris this weekend\",\n",
        "    \"Set alarm for 8 AM on Monday\",\n",
        "    \"Order a cappuccino and a croissant\",\n",
        "]\n",
        "\n",
        "# Batch inference for speed\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "results = []\n",
        "for i, utterance in enumerate(test_utterances, 1):\n",
        "    result = process_utterance(utterance)\n",
        "    results.append(result)\n",
        "\n",
        "    print(f\"{i}. Utterance: {result['utterance']}\")\n",
        "    print(f\"   Intent: {result['intent']} (confidence: {result['confidence']:.4f})\")\n",
        "\n",
        "    if result['slots']:\n",
        "        print(\"   Slots:\")\n",
        "        for slot_type, slot_value in result['slots']:\n",
        "            print(f\"     - [{slot_type}]: {slot_value}\")\n",
        "    else:\n",
        "        print(\"   Slots: None\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"\\n✓ Processed {len(test_utterances)} utterances in {elapsed:.2f}s\")\n",
        "print(f\"✓ Average: {elapsed/len(test_utterances)*1000:.1f}ms per utterance\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joeRK5RJu8p6",
        "outputId": "3c1dae63-4e88-47f9-e309-67d12d166975"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TESTING COMBINED NLU SYSTEM (A100)\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3702155083.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Enable autocasting for A100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Utterance: Wake me up at 6 AM tomorrow\n",
            "   Intent: alarm_set (confidence: 0.9998)\n",
            "   Slots:\n",
            "     - [time]: 6\n",
            "     - [time]: AM\n",
            "     - [date]: tomorrow\n",
            "----------------------------------------------------------------------\n",
            "2. Utterance: Book a table at an Italian restaurant at 7pm\n",
            "   Intent: recommendation_locations (confidence: 0.4810)\n",
            "   Slots:\n",
            "     - [business_type]: Italian restaurant\n",
            "     - [time]: 7pm\n",
            "----------------------------------------------------------------------\n",
            "3. Utterance: Play some jazz music by Miles Davis\n",
            "   Intent: play_music (confidence: 0.9987)\n",
            "   Slots:\n",
            "     - [music_genre]: jazz\n",
            "     - [artist_name]: Miles\n",
            "     - [artist_name]: Davis\n",
            "----------------------------------------------------------------------\n",
            "4. Utterance: Turn off the bedroom lights\n",
            "   Intent: iot_hue_lightoff (confidence: 0.9989)\n",
            "   Slots:\n",
            "     - [house_place]: bedroom\n",
            "----------------------------------------------------------------------\n",
            "5. Utterance: Remind me to call John next Friday\n",
            "   Intent: calendar_set (confidence: 0.9995)\n",
            "   Slots:\n",
            "     - [event_name]: call\n",
            "     - [person]: John\n",
            "     - [date]: next\n",
            "     - [date]: Friday\n",
            "----------------------------------------------------------------------\n",
            "6. Utterance: What's the weather in Paris this weekend\n",
            "   Intent: weather_query (confidence: 0.9995)\n",
            "   Slots:\n",
            "     - [place_name]: Paris\n",
            "     - [date]: this\n",
            "     - [date]: weekend\n",
            "----------------------------------------------------------------------\n",
            "7. Utterance: Set alarm for 8 AM on Monday\n",
            "   Intent: alarm_set (confidence: 0.9998)\n",
            "   Slots:\n",
            "     - [time]: 8\n",
            "     - [time]: AM\n",
            "     - [date]: Monday\n",
            "----------------------------------------------------------------------\n",
            "8. Utterance: Order a cappuccino and a croissant\n",
            "   Intent: iot_coffee (confidence: 0.9950)\n",
            "   Slots:\n",
            "     - [food_type]: cappuccino\n",
            "     - [food_type]: croissant\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "✓ Processed 8 utterances in 1.03s\n",
            "✓ Average: 128.7ms per utterance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Save Results\n",
        "# ============================================================\n",
        "\n",
        "output_path = f'{drive_path}/combined_nlu_results.json'\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n✓ Results saved to: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krlmWJP2vGfl",
        "outputId": "9437c7ec-75c9-401e-ebb7-2aa9721f5aa0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Results saved to: /content/drive/MyDrive/intent_project/combined_nlu_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INTERACTIVE NLU TESTING (A100)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nType utterances to test (or 'quit' to exit)\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter utterance: \").strip()\n",
        "\n",
        "    if user_input.lower() in ['quit', 'exit', 'q', '']:\n",
        "        print(\"\\n Testing complete!\")\n",
        "        break\n",
        "\n",
        "    result = process_utterance(user_input)\n",
        "\n",
        "    print(f\"\\nIntent: {result['intent']} ({result['confidence']:.2%})\")\n",
        "    if result['slots']:\n",
        "        print(\"Slots:\")\n",
        "        for slot_type, slot_value in result['slots']:\n",
        "            print(f\"  [{slot_type}]: {slot_value}\")\n",
        "    else:\n",
        "        print(\"Slots: None\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqzh9osAvMOy",
        "outputId": "2d3dba0d-8119-4ca5-e146-f5c29b68afb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "INTERACTIVE NLU TESTING (A100)\n",
            "======================================================================\n",
            "\n",
            "Type utterances to test (or 'quit' to exit)\n",
            "\n",
            "Enter utterance: రేపు జాన్ కి ఈమెయిల్ పంపమని నాకు గుర్తు చేయి.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3702155083.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Enable autocasting for A100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Intent: calendar_set (99.93%)\n",
            "Slots:\n",
            "  [date]: రేపు\n",
            "  [person]: జాన్\n",
            "\n",
            "Enter utterance: remind me to send an email tomorrow to john\n",
            "\n",
            "Intent: calendar_set (99.82%)\n",
            "Slots:\n",
            "  [date]: tomorrow\n",
            "  [date]: to\n",
            "  [person]: john\n",
            "\n",
            "Enter utterance: రేపు జాన్ కి ఒక ఈమెయిల్ పంపు.\n",
            "\n",
            "Intent: email_sendemail (99.97%)\n",
            "Slots:\n",
            "  [date]: రేపు\n",
            "  [person]: జాన్\n",
            "\n",
            "Enter utterance: quit\n",
            "\n",
            " Testing complete!\n"
          ]
        }
      ]
    }
  ]
}